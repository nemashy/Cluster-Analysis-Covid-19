---
title: \vspace{3.5in} Cluster Analysis Assignment
author: "Nyasha Mashanda"
date: "`r Sys.Date()`"
bibliography: "bibliography.bib"
output:
  bookdown::pdf_document2:
    keep_tex: true

---


\newpage 
\tableofcontents 
\listoffigures
\listoftables
\newpage
\pagebreak

\abstract

The novel COVID-19 corona virus is still not well understood and there are many open questions related to patterns in its spread. The goal of this assignment is to discover if there are any regional patterns that exist using cluster analysis.


The assignment uses COVD-19 pandemic data collected from the Our World In Data site [@WorldInData]. The data contains 29 indicators related to the
COVID-19 cases for 208 countries. The data set is updated daily from when the pandemic started. For this assignment, a subset of the
data will be used; this subset consists of all the information on the pandemic on the 02 of September
2020.


There are three kinds of clustering methods that will be explored in this analysis: hierarchical, partitioning and density based methods. In order to determine any regional patterns, the number of clusters will be limited to 6, resembling the six regions that are: Africa, Asia, Europe, North America, Oceania and South America.



```{r setup, include=FALSE}

# Loading all necessary packages or install if packages don't exist

knitr::opts_chunk$set(echo = TRUE)
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(cluster)) install.packages("cluster", repos = "http://cran.us.r-project.org")
if(!require(NbClust)) install.packages("NbClust", repos = "http://cran.us.r-project.org")
if(!require(fpc)) install.packages("fpc", repos = "http://cran.us.r-project.org")
if(!require(mice)) install.packages("mice", repos = "http://cran.us.r-project.org")
if(!require(VIM)) install.packages("VIM", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(visdat)) install.packages("visdat", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(missForest)) install.packages("missForest", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
```


```{r echo=FALSE, acronyms-tab, message=FALSE}

# Display this on the first page to see what and from where each variable was collected.
owid_covid_codebook <- read_csv("owid-covid-codebook.csv")
knitr::kable(owid_covid_codebook, digits = 5, caption = "Acronyms") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, border_right = T)

#View(owid_covid_codebook)
```


\newpage


\pagebreak


# What about standardising the data????

# Exploratory Data Analysis

## Importing the Data

```{r include=FALSE}

# Reading the data
df1 <- read_excel("owid-covid-data.xlsx", sheet = "Data")

# Check if data has been imported correctly
head(df1) 
tail(df1)

# Look at the structure of the data
str(df1)


```

The data was exported to a dataframe from an excel file "owid-covid-data.xlsx". The first step was to check if the data has been imported correctly using the head() and tail() functions. This file was confirmed to have been imported correctly. The next step included checking the structure of the dataframe and also the variables in the data (See variables in Table \@ref(tab:acronyms-tab)). Location, iso_code, continent and date were found to be in character format while the rest of the variables are numerical. The date variable is expected to be in a date format. However, since this column has only one date, the column will not be used for analysis and will be removed from the dataframe.

The next step was to visualize the distribution of the variables and this was done using box and whisker plots. This is a very important step as it may highlight outliers and incorrectly recorded values that are out of the expected range. 

## Distribution of variables

In plotting the distribution of numerical variables, the variables were divided into three groups. This makes it easier to analyse the variables. The variable in the first group are shown in Fig \@ref(fig:group-1).

```{r group-1, echo=FALSE, warning=FALSE, fig.cap="Group 1 of Variables"}

# Plot box and whisker for all variables

df1 %>%
  keep(is.numeric) %>%
  select(c(1:9)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
      # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=FALSE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )



```
Most variabled have many outleiers that make them seem as if thier values are less spread. This indicates that a clustering method that is less susceptible to outliers can be used.

Taking a deeper look into the data, it is immediately obvious that new_cases_smoothed_per_million might has an incorrectly recorded value given that it has a negative outlier. To deal with the negative value in new_cases_smoothed_per_million the data was explored to identify the observation with the negative value. This data point was found to be belonging to Luxembourg. Upon further investigation, Luxembourg seems to have negative values for new_cases_smoothed and new_cases_smoothed_per_million. This is probably because the new_cases_smoothed_per_million is derived from new_cases_smoothed.


```{r include=F}
# Find the obserservation with negative value
Lx = df1 %>% 
  filter(new_cases_smoothed_per_million < 0)
head(Lx)
```


```{r include=FALSE, warning=FALSE}


# Replace negative value with continental mean
df1  = df1 %>% 
  group_by(continent) %>%
  mutate(new_cases_smoothed_per_million = ifelse(new_cases_smoothed_per_million < 0,
                                                 mean(new_cases_smoothed_per_million),
                                                 new_cases_smoothed_per_million),
         new_cases_smoothed = ifelse(new_cases_smoothed < 0,
                                     mean(new_cases_smoothed),
                                     new_cases_smoothed))
```


 





For variables in Fig \@ref(fig:group-2, there are numerous outliers in each of the variables. 


```{r group-2, echo=FALSE, warning=FALSE, fig.cap="Group 2 of Variables"}
df1 %>%
  keep(is.numeric) %>%
  select(c(10:16)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
            # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=FALSE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )

```

Median_age and aged_65_older values are much more spread.  The numbers seem resasonable given that a smaller percentage of the population is age above 65 and usually the median age of most countries is expected to be below 50 given that there usually more young people than older people in country. Countries like Japan and Italy seem to have the highest percentage of older people with a median age of 48.2 and 47.9. The Niger and Uganda have the youngest population with a median age of 16.1 and 15.4. In general European countries seems to have an older society while Africa countries have a much younger population.

Monaco and Singapore have the highest population densities while China has the highest population. As a result these countries stand out as outliers with regards to population variables.

New Deaths per million are highest in north and South America.

New cases smoothed per million show a negative number(outlier). This is most likely a entry error and the values will be replaced with a regional average.

The histograms also suggest that there is a negative value in new_cases_smoothed_per_million. 


```{r echo=FALSE, warning=FALSE, fig.cap="Group 3 of Variables"}

df1 %>%
  keep(is.numeric) %>%
  select(c(17:25)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
            # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=FALSE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )

```

The observations for these variables look more spread with less outliers. The variables also suggest that there are generally more male smokers than female smokers. Furthermore, the data show that it is mostly european countries that are wealthy or less poor. The numbers seem to be in the expected ranges.

## Missing values

```{r include=F}
summary(df1)
```

Running a summary on the dataframe shows that there are various columns with missing data. 

```{r , echo=FALSE}
missing_data_visual <- vis_miss(df1, sort_miss = T) # Visualise to see rows/columns with missing data
#missing_data_visual

aggr_plot <- VIM::aggr(df1, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df1), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"), plot = F)
plot(aggr_plot, cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))

# Creating table
missing_data_table = aggr_plot$missings %>%
  filter(Count > 0) %>%
  arrange(desc(Count)) %>%
  mutate(Percentage = Count/208)

```
```{r missing-data-table, echo=FALSE}
knitr::kable(missing_data_table, digits = 5, caption = "Missing data visualisation") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(3, border_right = T)
```

As show in (Table \@ref(tab:missing-data-table)), a number of variables have a very high number of missing values including handwashing_facilities, extreme_poverty, male smokers and female smokers. It is generally not ggod working with data or columns that have a high percentage of missing data. Therefore various methods of imputing the missing data will be observed in the next section.

## Imputation by data scrapping

There are various sources that were used to collect the data including the European Center for Disease Prevention and Control. However some of the data can be found on wikipedia and various internet sources. Consequently, looking for the data before applying an imputation algorithms is much better. Although the data was not exactly be the same, in most cases the values were found to be very similar thereby justify the imputation.



```{r include=FALSE}

# Determine the percentage of rows with missing data
missing_rows <- sum(!complete.cases(df1))
total_rows <- nrow(df1)
percentage_of_missing_rows <- missing_rows/total_rows * 100
percentage_of_missing_rows

```

Population density missing values were imputed using the data from wikipedia [@PopulationDensity]. The data for all the countries with missing values was found except for the Falkland Islands. 



```{r include=F}

# Imputing population density missing values.

# The process includes finding countries with
# missing data in a column of interest and 
# then impute the missing data with data from
# a source

df2 <- df1[is.na(df1$population_density),] # Dataframe with missing value in a column
missing_data_countries <-  df2$location
#missing_data_countries
imputations <- c(19.83, 2.25, 92, 652.11, 804.14, 898.28, 924.49, 140.13, 65.64, NA, 0.21)
position_in_vector <- which(df1$location %in% missing_data_countries)
df1$population_density[position_in_vector] <-  imputations # Imputing the data

sum(is.na(df1$population_density)) # Check if data has been imputed

```

Median age missing values were imputed using the data found from the CIA [@MedianAge]. Using this site only three countries had values for median age that could not be found.



```{r include=F}

# Imputing median_age missing values.

df3 <- df1[is.na(df1$median_age),]
missing_data_countries <-  df3$location
#missing_data_countries
median_ages  <- c(46.2, 37.2, 35.5, 44.3, 44.6, 37.5, 30.5, 43.7, 55.4, 45.2, NA, 35.7, 43.6, NA, 37.2, 40.5, 34.9, 34.3, 34.8, 36.5, 41.1, 34.6, 32.8, NA)
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$median_age[position_in_vector] <-  imputations


```


aged_65_older missing values were imputed using 2019 data from wikipedia [@Aged65]. Only data for Syria, Taiwan and British Virgin Islands. The rest of the countries values could not be imputed using this method. 
```{r include=FALSE}

# Imputing aged_65_older missing values.

df4 <- df1[is.na(df1$aged_65_older),]
missing_data_countries <-  df4$location
#missing_data_countries
median_ages  <- c(NA, 4.3, 13.86, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 18.6, NA, NA, NA, NA, NA, NA, NA, NA, NA )
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$aged_65_older[position_in_vector] <-  median_ages

sum(is.na(df1$aged_65_older))

```

Median age above 75 years old was ignored since there was not much data on the internet regarding this variable.

Extreme poverty is a population with an income of less than $1.90 a day. From the data the Share of the population living in extreme poverty, most recent year available since 2010. The data from wikipedia [@Poverty]. The data is very similar to that available without missing values, therefore it will be better to use the data for imputing. 

Most countries with 0 percent were having missing values

```{r include=FALSE}

# Imputing extreme_poverty missing values.

df4 <- df1[is.na(df1$extreme_poverty),]
missing_data_countries <-  df4$location
#missing_data_countries
extreme_pov <- rep(NA, length(missing_data_countries))

extreme_pov[c(1, 2, 3, 4, 8, 9, 11, 12, 13, 16, 20, 22, 23, 25, 29, 32, 34, 35, 36, 38, 39, 40, 47, 48, 50, 51, 52, 59, 69, 76, 86, 87, 88)] <- c(47.6, 16.1, 3.2, 66.3, 49.7, 53.5, 42.7, 14.9, 42, 0, 0.2, 0, 7.3, 6.1, 1.7, 62.1, 0, 0, 0, 0, 0, 0, 0, 0, 5.5, 0, 0, 13.9, 1.7, 3.4, 14, 23.4, 10.2)

position_in_vector <- which(df1$location %in% missing_data_countries)
df1$extreme_poverty[position_in_vector] <-  extreme_pov

sum(is.na(df1$extreme_poverty))
```

Cardiovascular death rate was skipped due to insufficient data to fill in the missing values.
```{r include=FALSE}
df4 <- df1[is.na(df1$cardiovasc_death_rate),]
missing_data_countries <-  df4$location
#missing_data_countries

```

```{r include=FALSE}

# Imputing life_expectancy missing values.

df4 <- df1[is.na(df1$life_expectancy),]
missing_data_countries <-  df4$location
#missing_data_countries

life_exp <- c(82.6, 80.6, 71.95)
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$life_expectancy[position_in_vector] <-  life_exp

sum(is.na(df1$life_expectancy))
```


female and male smokers info not enough
handwashing facilities skipped because data is not enough.
hospital bed per thousand informatiion was found to be too old for example Chad which had data for 2005. Given that there are many years between the time the data was collected and this year the data was ignored.

Only three observations were found to have missing life expectancy values. Imputation was done using the world bank data on life expectancy [@LifeExpectancy]. The life expectancy of Guernsey, Jersey and Kosovo were found to be 82.6, 80.6, 71.95 years respectively.


No data was found on the internet woith regards to the rest of the variables on their missing data. There alternative methods had to be applied.


With regards to population density, extreme poverty, gdp per capita, hospital beds per thousand, aged 65 and older and aged 70 and older, a better imputation method would be replacing missing values with column regional means given that countries in the same regions are more likely to have the same of these variables.


```{r include=FALSE}
# For the gdp per capita the missing values will be replaced by the mean for each region under the assumption that countries that are closer to each other usually have similar rates of poverty measures/gdp_per_capita, hospital beds per thousand, aged_65_older and aged_70_older


df6 <- df1 %>%
    group_by(continent) %>%
    mutate(
        population_density=ifelse(is.na(population_density),mean(population_density,na.rm=TRUE),population_density),
        extreme_poverty=ifelse(is.na(extreme_poverty),mean(extreme_poverty,na.rm=TRUE),extreme_poverty),
        gdp_per_capita=ifelse(is.na(gdp_per_capita),mean(gdp_per_capita,na.rm=TRUE),gdp_per_capita),
        hospital_beds_per_thousand=ifelse(is.na(hospital_beds_per_thousand),mean(hospital_beds_per_thousand,na.rm=TRUE),hospital_beds_per_thousand),
        aged_65_older=ifelse(is.na(aged_65_older),mean(aged_65_older,na.rm=TRUE),aged_65_older),
        aged_70_older=ifelse(is.na(aged_70_older),mean(aged_70_older,na.rm=TRUE),aged_70_older)
  
    )
```


Due to hand_washing_facilities having missing data greater than 70%, the variable was removed since it is most likely that the imputed values will be far from the actual values.

After imputation using the data scrapping and regional averages, the looks like below:
```{r include=F}

# Remove handwashing_facilities since there is not enough 
# data for imputation

df7 <- select(df6,-handwashing_facilities)
aggr_plot <- VIM::aggr(df7, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df7), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"), plot = T)

```
```{r include=FALSE}
plot(aggr_plot, cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

Now the highest percentage of missing values is found in male smokers and female smokers having 34% and 33 % missing values. For the these variables and other that have missing values different imputation algorithms will be explored and the one that gives the best results will be chosen.

```{r include=FALSE}

# Visualize the proportion of missing values in rows/columns

aggr_plot <- VIM::aggr(df7, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df7), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```
The results show that handwashing_facilites is missing 56.25% of the values followed by extreme_poverty missing 42.3% of the values. In total, 78.4% of the samples have missing values.


## Mice

The mice package allows multivariate imputation by chained equations. MICE assumes that the missing data are Missing at Random (MAR), which means that the propensity for a data point to be missing is not related to the missing data but is related to some of the observed data [@MissingData]. In this section, the MICE package will be used on the remaining variables with missing data that are cardiovasc_death_rate, diabetes prevalence, female and male smokers.

The number of multiple imputations was set to 5, method was set to classification and regression trees and the maximum number of iterations set to 500. 5 datasets were created and the goodness of fit is shown in the plots below. The graphs of imputed values(red) closely resembles that of the available values(blue). This shows that the imputation process was good.


```{r include=FALSE}


# Impute missing data using the mice package

#imputed_data <- mice(df7,m=5,maxit=50, method='cart', seed=500)
#summary(imputed_data)
#save(imputed_data, file = "replaced_missing_data.RData")
load("replaced_missing_data.RData")

sum(is.na(imputed_data)) # Check if imputations have been done
```

```{r echo=FALSE}
# Check the quality of imputations

mice::densityplot(imputed_data)

```


Going forward the second dataset was randomly selected from the five datasets. This will not affect the outcome significantly given that the datasets have very similar properties. The continent, date and iso_code variables were removed from the dataset as they will not be useful in the final analysis. The dataset was named mice_data_for_clustering.

```{r include=FALSE}
# Select one of the imputed datasets

completedData <- mice::complete(imputed_data, 2)
#View(completedData)
#str(completedData)

# Remove isocode, continent and data ????
# Is it necessary continentt

df8 <- completedData[,c(-2,-3,-4)]
rownames(df8) <- df8$location

# Removing country names
mice_data_for_clustering <- df8[,-1]

```

## Hmisc
Hmisc is a multiple purpose package useful for data analysis, high – level graphics, imputing missing values, advanced table making, model fitting & diagnostics (linear regression, logistic regression & cox regression) etc. Hmisc assumes linearity in the variables being predicted. The package was also used to impute values on the missing values.

mi (Multiple imputation with diagnostics) package provides several features for dealing with missing values. Like other packages, it also builds multiple imputation models to approximate missing values. And, uses predictive mean matching method like the mice package.

There is also the Amelia package and mi package that are used for imputation but these will not be considered for this analysis.



The percentage of population aged_65_older looks to have regional patterns therefore it is justified to use regional medians/averages for imputation. Since aged_70_older can is closely related to aged_65_older it is decided to use the same method.



```{r include=FALSE}
# Impute missing data using the Hmisc package

df7.mis <- prodNA(df7, noNA = 0.1)

impute_arg <- aregImpute(~ cardiovasc_death_rate 
                         + diabetes_prevalence + female_smokers + male_smokers, data = df7.mis, n.impute = 5)

completeData2 <- impute.transcan(impute_arg, imputation=1, data=df7.mis, list.out=TRUE,pr=FALSE, check=FALSE) 

#head(completeData2)
#class(completeData2)

df_Hmisc = df7 # Make a copy of df7

df_Hmisc$cardiovasc_death_rate = completeData2$cardiovasc_death_rate
df_Hmisc$diabetes_prevalence = completeData2$diabetes_prevalence
df_Hmisc$female_smokers = completeData2$female_smokers
df_Hmisc$male_smokers = completeData2$male_smokers

df_Hmisc <- completedData[,c(-2,-3,-4)]
rownames(df_Hmisc) <- df_Hmisc$location

# Removing country names
df_Hmisc_for_clustering <- df_Hmisc[,-1]

#summary(df_Hmisc_for_clustering)
```

The imputations were made on the dataset and the resulting dataset was named df_Hmisc_for_clustering. The two datasets were used for clustering and the results were compared.


# Cluster Analysis

In cluster analysis, the number of clusters of interest is 6 given that we have six distinct regions in the data that are Africa, Asia, Europe, North America, Oceania and South America. In order to quantify the goodness of fit for each clustering method, the average silhouette method which determines the quality of the clustering. The optimum number of clusters in a dataset will have the maximum average silhouette width.

## Heirachical clustering

Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. In this excercise complete, single, average, median and centroid linkage were tested.

The results from heirachical clustering are shown in :



```{r include=FALSE}
data.out <- mice_data_for_clustering
```


The randomizing, re-running, averaging and final run is a very good advice.




```{r include=F}
# Hierarchical clustering
# Compute pairwise distance matrices
dist.out <- dist(data.out,
                 method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.out,
             method = "complete")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc_single <- hclust(dist.out,
             method = "single")
# Visualization of hclust
plot(hc_single, labels = F,-1)
plot(silhouette(cutree(hc_single,6),dist.out))



hc <- hclust(dist.out,
             method = "average")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "median")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "centroid")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))


```


```{r echo=F}
hc_results <- read_excel("HeirachicalResults.xlsx")
knitr::kable(hc_results, digits = 5, caption = "HCClust") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, border_right = T)

tt = tibble(Country = hc[["labels"]], Group = silhouette(cutree(hc_single,6),dist.out)[,1])

```

The results show that single linkage had the best results with a average silhouette width of 0.83. China 3, India 4, Indonesia 5, Pakistan 2, Brazil 2, US 6 are outliers in this data. The rest of the countries fall within the same group. Although the average silhouette width is high, the results do not demonstrate any regional patterns for the six continents.

## K-means

```{r echo=F}

# Using k-means clustering
k.max <- 10

# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
set.seed(100)
for(i in 2:k.max){
  
  km.res <- kmeans(data.out, centers = i, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(data.out))
  sil[i] <- mean(ss[, 3])
  if (i == 2) {
    clusters_2 = km.res$cluster
  }
  if (i == 6) {
    clusters_6 = km.res$cluster
  }
}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))

knitr::kable(clusters_6_dataframe, digits = 3, caption = "K-means") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(4, border_right = T)


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])
```
K-means clustering results show that the average silhouette width for six clusters is 0.677787 for 6 clusters. The diagram suggest that the best number of cluster in the data is 2 with an ASW of 0.9736113. India and China form part of their own clusters while other countries are in the second cluster. More than 80% of african countries fall in groups 4 and 5, around 70 % of Asian countries fall in cluster 4 and 5, 88% of North American countries fall in cluster 4, 87% of Oceania countries fall in clusters 4, 76 of south American Countries fall in cluster 4 and 6. This information seem to suggests that there are mainly two clusters that is 4 and 5. This is further supported by the ASW of 0.9736113 for two clusters.


## K-Mediods

```{r echo=F}
# Using k-medoids clustering
set.seed(100)
pam.out <- pam(data.out, 6)


# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  pam.out <- pam(data.out, i)
  sil[i] <- pam.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = pam.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = pam.out$clustering
  }

}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])
#class_avg_width

```

The ASW for six clusters is 0.610983 while that of 2 clusters is the maximum at 0.9736113. The information also suggest that the optimum number of clusters is 2. Over 90% of African countries belong to group 1, 2 and 3, 75% of Asian countries belong to group 1, 2 and 3. 86% of European countries belong to group 2 and 3, 91% of North American countries belong to group 2 and 3, 75% of Oceania countries belong to group 3 and more than 90% of South American countries belong to group 1, 2 and 3. These results show that there are at most three main clusters in the data and there is no specific regional pattern that can be seen. The PAM method also suggests that the optimum number of clusters in 2.



## Clara
```{r echo=FALSE}
# Using Clara
clara.out <- clara(data.out, 6, samples=10)

# Using k-medoids clustering
set.seed(100)


# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  clara.out <- clara(data.out, i, samples=10)
  sil[i] <- clara.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = clara.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = clara.out$clustering
  }

}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])
#class_avg_width

```
More than 91% of African countries are found in group 1, 2 and 3. More than 77% of Asian countries are found in group 1, 2 and 3. More than 76% of countries in Europe are found in groups 2 and 3. More than 91% of North American countries are found in groups 2 and 3. More than 87% of Oceanian countries are found in groups2 and 3 while more than 90% of South American countries are found in group 1, 2 and 3. Again, the information suggest that there are at most the main clusters in the data. 

Density based scanning was tested and in order to get six main clusters on the data the epsilon value was set to 47 000 and the min points to 3. This AWS was -0.21 which indicates that the clusters are of poor quality and objects are less similar in their own cluster. This may well mean that the data either has too many of too few clusters. The DBSCAN method also pick up many outliers in the data and very few countries belonging to groups. This method will not be explored for further analysis given that it does not give any good results.
```{r include=FALSE}
# Using DBSCAN
t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 47000, MinPts =3 )
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")
```


```{r include=FALSE}
data.out <- df_Hmisc_for_clustering 
```


The randomizing, re-running, averaging and final run is a very good advice.


```{r echo=F}

# Using k-means clustering
k.max <- 10

# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
set.seed(100)
for(i in 2:k.max){
  
  km.res <- kmeans(data.out, centers = i, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(data.out))
  sil[i] <- mean(ss[, 3])
  if (i == 2) {
    clusters_2 = km.res$cluster
  }
  if (i == 6) {
    clusters_6 = km.res$cluster
  }
}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))



# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])

```


```{r include=F}
# Hierarchical clustering
# Compute pairwise distance matrices
dist.out <- dist(data.out,
                 method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.out,
             method = "complete")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc_single <- hclust(dist.out,
             method = "single")
# Visualization of hclust
plot(hc_single, labels = F,-1)
plot(silhouette(cutree(hc_single,6),dist.out))



hc <- hclust(dist.out,
             method = "average")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "median")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "centroid")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))
```



```{r echo=F}
# Using k-medoids clustering
set.seed(100)
pam.out <- pam(data.out, 6)


# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  pam.out <- pam(data.out, i)
  sil[i] <- pam.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = pam.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = pam.out$clustering
  }

}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])
#class_avg_width
```

```{r include=FALSE, echo=F}
# Using Clara
clara.out <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clara.out), col = 2:3, main = "Silhouette plot")



# Using k-medoids clustering
set.seed(100)


# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  clara.out <- clara(data.out, i, samples=10)
  sil[i] <- clara.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = clara.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = clara.out$clustering
  }

}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])
#class_avg_width
```
The dataset is very small and this results in Clara performing worse than PAM.


```{r include=FALSE}
# Using DBSCAN
t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 47000, MinPts =3 )
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")
```

# Dimension reduction

A correlation plot shows that a number of variables in the dataset are highly correlated. Therefore, the size of the dataset can be reduced but keeping most of the information in the dataset using dimension reduction techniques. In this exercise PCA will be used to reduce the size of the data and clustering will be performed on the reduced data.

```{r echo=FALSE}
df_correlation_plot <- df_Hmisc_for_clustering %>%
  keep(is.numeric)

correlation_matrix <- cor(df_correlation_plot)
corrplot(correlation_matrix, method="circle", tl.pos='n')
```


```{r include=FALSE}
# Attempt clustering after dimension reduction
# Using PCA

pca.out <- princomp(data.out, cor=T,scores=T)
Variance<-(pca.out$sdev)^2
max_Var<-round(max(Variance),1)
Components<-c(1:25)
Components<-as.integer(Components)

df10 =pca.out$scores
df11 =  df10[,c(1:4)]

# Average silhouette method for k-means clustering
k.max <- 10
data.out <- df11
# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){

km.res <- kmeans(data.out, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(data.out))
sil[i] <- mean(ss[, 3])
}
# Plot the average silhouette width
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

# Partition around medoids
pam.out <- pam(data.out, 4)
head(pam.out$cluster)
plot(pam.out)

pam.out$silinfo$avg.width

k.max = 20
for (k in 2:k.max) {
  pam.out <- pam(data.out, k)
  sil[k] = pam.out$silinfo$avg.width
}

plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

print(sil[6])
print(sil[2])

# Using Clara
clarax <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clarax), col = 2:3, main = "Silhouette plot")

clarax$silinfo$avg.width

for (k in 2:k.max) {
  pam.out <- pam(data.out, k)
  sil[k] = pam.out$silinfo$avg.width
}

plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 2.7, MinPts = 1)
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")

```
The scree plot shows that most of the variance in the data can be found in the first 4 principal componenents. The first 4 principal compnents were taken to represent the whole dataset.

The ASW for k-means clustering for all cluster numbers was found to be generally lower than that of the unreduced dataset. For 6 clusters, the ASW is 0.3549331 and for 2 clusters 0.7914952. The value for six clusters is very low to suggest any reasonable regional patterns given that the AWS is low.


```{r echo=F}

# Using k-means clustering
k.max <- 10

# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
set.seed(100)
for(i in 2:k.max){
  
  km.res <- kmeans(data.out, centers = i, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(data.out))
  sil[i] <- mean(ss[, 3])
  if (i == 2) {
    clusters_2 = km.res$cluster
  }
  if (i == 6) {
    clusters_6 = km.res$cluster
  }
}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))



# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

# print(sil[6])
# print(sil[2])

```


```{r include=F}
# Hierarchical clustering
# Compute pairwise distance matrices
dist.out <- dist(data.out,
                 method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.out,
             method = "complete")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc_single <- hclust(dist.out,
             method = "single")
# Visualization of hclust
plot(hc_single, labels = F,-1)
plot(silhouette(cutree(hc_single,6),dist.out))



hc <- hclust(dist.out,
             method = "average")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "median")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "centroid")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))
```



```{r}
hc_results <- read_excel("HeirachicalResultsHmisc.xlsx")
knitr::kable(hc_results, digits = 5, caption = "HCClust") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, border_right = T)

tt = tibble(Country = hc[["labels"]], Group = silhouette(cutree(hc_single,6),dist.out)[,1])
```


The AWS for all the heirachical clustering methods are less than or equal to 0.5 for 6 clusters. This also shows that the counties are less likely to be divided ino six continental regions thereby no evidence for regional patterns.


```{r echo=F}
# Using k-medoids clustering
set.seed(100)
pam.out <- pam(data.out, 6)

# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  pam.out <- pam(data.out, i)
  sil[i] <- pam.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = pam.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = pam.out$clustering
  }

}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))


# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

#print(sil[6])
#print(sil[2])

```
The K-mediods method gives an ASW of 0.32 for six clusters. It also suggests that there are probably more than six clusters for the data.


```{r echo=F}
# Using Clara
clara.out <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clara.out), col = 2:3, main = "Silhouette plot")



# Using k-medoids clustering
set.seed(100)


# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){
  clara.out <- clara(data.out, i, samples=10)
  sil[i] <- clara.out[["silinfo"]][["avg.width"]]
  if (i==6) {
    class_avg_width = clara.out[["silinfo"]][["clus.avg.widths"]]
    clusters_6 = clara.out$clustering
  }
}

clusters_6_dataframe = as.data.frame(clusters_6) %>%
  mutate(continent = df1$continent) %>%
  group_by(continent, clusters_6) %>%
  summarise(count = n()) %>%
  group_by(continent) %>%
  mutate(Percentage = round(count*100/sum(count), digits = 2))

# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

print(sil[6])
print(sil[2])
class_avg_width

```
The dataset is very small and this results in Clara performing worse than PAM.


In general, reducing the dimension of the data led to a worse performance in the clustering algorithm. Therefore, it is best to use the full dataset for clustering.


# Conclusion

The results show that the best algorithm is xys
The optimum number of clusters is
The following countries are outliers
Regional patterns on a continental scale do not exist in the data


This code was optimised for reading

# References

