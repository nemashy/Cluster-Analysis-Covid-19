---
title: \vspace{3.5in} Cluster Analysis Assignment
author: "Nyasha Mashanda"
date: "`r Sys.Date()`"
bibliography: "bibliography.bib"
output:
  bookdown::pdf_document2:
    keep_tex: true

---

\newpage 
\tableofcontents 
\listoffigures
\listoftables
\newpage


```{r setup, include=FALSE}

# Loading all necessary packages or install if packages don't exist

knitr::opts_chunk$set(echo = TRUE)
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(cluster)) install.packages("cluster", repos = "http://cran.us.r-project.org")
if(!require(NbClust)) install.packages("NbClust", repos = "http://cran.us.r-project.org")
if(!require(fpc)) install.packages("fpc", repos = "http://cran.us.r-project.org")
if(!require(mice)) install.packages("mice", repos = "http://cran.us.r-project.org")
if(!require(VIM)) install.packages("VIM", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(visdat)) install.packages("visdat", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(missForest)) install.packages("missForest", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")

```





```{r acronyms-tab}
library(kableExtra)
# Display this on the first page to see what and from where each variable was collected.
owid_covid_codebook <- read_csv("owid-covid-codebook.csv")
knitr::kable(owid_covid_codebook, digits = 5, caption = "Acronyms") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, border_right = T)

#View(owid_covid_codebook)
```


\newpage
\abstract

The novel COVID-19 corona virus is still not well understood and there are many open questions related to patterns in its spread. The goal of this assignment is to discover if there are any regional patterns that exist using cluster analysis.


The assignment uses COVD-19 pandemic data collected from the Our World In Data site [@WorldInData]. The data contains 29 indicators related to the
COVID-19 cases for 208 countries. The data set is updated daily from when the pandemic started. For this assignment, a subset of the
data will be used; this subset consists of all the information on the pandemic on the 02 of September
2020.


There are three kinds of clustering methods that will be explored in this analysis: hierarchical, partitioning and density based methods. In order to determine any regional patterns, the number of clusters will be limited to 6, resembling the six regions that are: Africa, Asia, Europe, North America, Oceania and South America.



\pagebreak


# What about standardising the data????

# Exploratory Data Analysis

```{r }

# Reading the data
df1 <- read_excel("owid-covid-data.xlsx", sheet = "Data")

# Check if data has been imported correctly
head(df1) 
tail(df1)

# Look at the structure of the data
str(df1)


```

The data was exported to a dataframe from an excel file "owid-covid-data.xlsx". The first step is to check if the data has been imported correctly using the head() and tail() functions. This was confirmed by running the code. The next step included checking the structure of the dataframe and also the variables in the data (See variables in Table \@ref(tab:acronyms-tab)). Location, iso_code, continent and date were found to be in character format while the rest of the variables are numerical. This is fine expcept for the date which should be in a date format. However, since this column has only one date, the column will not be used for ana;ysis and will be removed from the dataframe.

The next step is to visualize the distribution of the variables and this will be done using box and whisker plots. 



## Distribution



The data shows that handwashing_facilties, extreme poverty, male smokers and female smokers are among the columns with the highest percentages of missing data.




```{r }

# Plot box and whisker for all variables

df1 %>%
  keep(is.numeric) %>%
  select(c(1:8)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
      # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=TRUE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )



```

The first seven figures shpw that most of the data is concerntrated very close to less than a thousand except for total_cases_per_million which is more spread and between 0 and 5000. All the variables contain numerous outliers and this shows that we might need a clustering method that is robust and not affected by outliers.

```{r}
df1 %>%
  keep(is.numeric) %>%
  select(c(9:16)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
            # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=TRUE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )

```
Almost half of the variables
Median_age and aged_65_older values are much more spread.  The numbers seem resasonable given that a smaller percentage of the population is age above 65 and usually the median age of most countries is expected to be below 50 given that there usually more young people than older people in country. Countries like Japan and Italy seem to have the highest percentage of older people with a median age of 48.2 and 47.9. The Niger and Uganda have the youngest population with a median age of 16.1 and 15.4. In genral European countries seems to have an older society while Africa countries have a much younger population.

Monaco and Singapore have the highstest population densities while China has the highest population. As a result these countries stand out as outliers with regards to population variables.

New Deaths per million are hightest in north and south america.


New cases smoothed per million show a negative number(outlier). This is most likely a entry error and the values will be replaced with a regional average.


The histograms also suggest that there is a negative value in new_cases_smoothed_per_million. 

To deal with the negative value in new_cases_smoothed_per_million the data was expolred to indentify the observation with the negative value. Upon inspection it was also found that the observation also had a negative value for new_cases_smoothed. The two values were replaced with the regional means.
 
```{r include=FALSE}
# Find the obserservation with negative value
df1 %>% 
  filter(new_cases_smoothed_per_million < 0)

# Replace negative value with continental mean
df1  = df1 %>% 
  group_by(continent) %>%
  mutate(new_cases_smoothed_per_million = ifelse(new_cases_smoothed_per_million < 0,
                                                 mean(new_cases_smoothed_per_million),
                                                 new_cases_smoothed_per_million),
         new_cases_smoothed = ifelse(new_cases_smoothed < 0,
                                     mean(new_cases_smoothed),
                                     new_cases_smoothed))
  
```

Upon further investigation, Luxembourg seems to have negative values for new_cases_smoothed and new_cases_smoothed_per_million. This is probably because the new_cases_smoothed_per_million is derived from new_cases_smoothed. 

```{r}

df1 %>%
  keep(is.numeric) %>%
  select(c(17:25)) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_boxplot(
            # custom boxes
        color="blue",
        fill="blue",
        alpha=0.2,
        
        # Notch?
        notch=TRUE,
        notchwidth = 0.8,
        
        # custom outliers
        outlier.colour="black",
        outlier.fill="black",
        outlier.size=2
    )

```

The observations for these variables look more spread with less outliers. The variables also suggest that there are generally more male smokers than female smokers. Furthermore, the data show that it is mostly european countries that are wealthy or less poor. The numbers seem to be in the expected ranges.

## Missing values



```{r include=F}
summary(df1)
```

Running a summary on the dataframe shows that there are various columns with missing data. 

```{r missing_data_tab}
missing_data_visual <- vis_miss(df1, sort_miss = T) # Visualise to see rows/columns with missing data
#missing_data_visual

aggr_plot <- VIM::aggr(df1, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df1), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"), plot = F)
plot(aggr_plot, cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))

# Creating table
missing_data_table = aggr_plot$missings %>%
  filter(Count > 0) %>%
  arrange(desc(Count)) %>%
  mutate(Percentage = Count/208)

knitr::kable(missing_data_table, digits = 5, caption = "Missing data visualisation") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(3, border_right = T)
```

As show in Table \@ref(missing_data_tab), a number of variables have a very high number of missing values including handwashing_facilities, extreme_poverty, male smokers and female smokers. It is generally not ggod working with data or columns that have a high percentage of missing data. Therefore various methods of imputing the missing data will be observed in the next section.

## Imputation by data scrapping

There are various sources that were used to collect the data including the European Center for Disease Prevention and Control. However some of the data can be found on wikipedia and various internet sources. Consequently, looking for the data before applying an imputation algorithms is much better. Although the data was not exactly be the same, in most cases the values were found to be very similar thereby justify the imputation.



```{r include=FALSE}

# Determine the percentage of rows with missing data
missing_rows <- sum(!complete.cases(df1))
total_rows <- nrow(df1)
percentage_of_missing_rows <- missing_rows/total_rows * 100
percentage_of_missing_rows

```
Over 78% of rows have missing data therefore simply omitting the rows with missing will lead to a loss a huge size of important data.

Population density missing values were imputed using the data from wikipedia [@PopulationDensity]. The data for all the countries with missing values was found except for the Falkland Islands. 



```{r }

# Imputing population density missing values.

# The process includes finding countries with
# missing data in a column of interest and 
# then impute the missing data with data from
# a source

df2 <- df1[is.na(df1$population_density),] # Dataframe with missing value in a column
missing_data_countries <-  df2$location
#missing_data_countries
imputations <- c(19.83, 2.25, 92, 652.11, 804.14, 898.28, 924.49, 140.13, 65.64, NA, 0.21)
position_in_vector <- which(df1$location %in% missing_data_countries)
df1$population_density[position_in_vector] <-  imputations # Imputing the data

sum(is.na(df1$population_density)) # Check if data has been imputed

```

Median age missing values were imputed using the data found from the CIA [@MedianAge]. Using this site only three countries had values for median age that could not be found.



```{r}

# Imputing median_age missing values.

df3 <- df1[is.na(df1$median_age),]
missing_data_countries <-  df3$location
#missing_data_countries
median_ages  <- c(46.2, 37.2, 35.5, 44.3, 44.6, 37.5, 30.5, 43.7, 55.4, 45.2, NA, 35.7, 43.6, NA, 37.2, 40.5, 34.9, 34.3, 34.8, 36.5, 41.1, 34.6, 32.8, NA)
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$median_age[position_in_vector] <-  imputations


```


aged_65_older missing values were imputed using 2019 data from wikipedia [@Aged65]. Only data for Syria, Taiwan and British Virgin Islands. The rest of the countries values could not be imputed using this method. 
```{r include=FALSE}

# Imputing aged_65_older missing values.

df4 <- df1[is.na(df1$aged_65_older),]
missing_data_countries <-  df4$location
#missing_data_countries
median_ages  <- c(NA, 4.3, 13.86, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 18.6, NA, NA, NA, NA, NA, NA, NA, NA, NA )
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$aged_65_older[position_in_vector] <-  median_ages

sum(is.na(df1$aged_65_older))

```

Median age above 75 years old was ignored since there was not much data on the internet regarding this variable.


Searching the source of the data maps to a site: https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-codebook.csv which says the gdp_per_capita 	Gross domestic product at purchasing power parity (constant 2011 international dollars), most recent year available. Given that the most recent data from the world bank (https://databank.worldbank.org/source/jobs/Series/NY.GDP.PCAP.PP.KD#) which matches this data is for the year 2016, the data was used for imputing the data. Note the data is not available




Extreme poverty is a population with an income of less than $1.90 a day. From the data the Share of the population living in extreme poverty, most recent year available since 2010. The data from wikipedia [@Poverty]. The data is very similar to that available without missing values, therefore it will be better to use the data for imputing. 

Most countries with 0 percent were having missing values

```{r include=FALSE}

# Imputing extreme_poverty missing values.

df4 <- df1[is.na(df1$extreme_poverty),]
missing_data_countries <-  df4$location
#missing_data_countries
extreme_pov <- rep(NA, length(missing_data_countries))

extreme_pov[c(1, 2, 3, 4, 8, 9, 11, 12, 13, 16, 20, 22, 23, 25, 29, 32, 34, 35, 36, 38, 39, 40, 47, 48, 50, 51, 52, 59, 69, 76, 86, 87, 88)] <- c(47.6, 16.1, 3.2, 66.3, 49.7, 53.5, 42.7, 14.9, 42, 0, 0.2, 0, 7.3, 6.1, 1.7, 62.1, 0, 0, 0, 0, 0, 0, 0, 0, 5.5, 0, 0, 13.9, 1.7, 3.4, 14, 23.4, 10.2)

position_in_vector <- which(df1$location %in% missing_data_countries)
df1$extreme_poverty[position_in_vector] <-  extreme_pov

sum(is.na(df1$extreme_poverty))
```

Cardiovascular death rate was skipped due to insufficient data to fill in the missing values.
```{r include=FALSE}
df4 <- df1[is.na(df1$cardiovasc_death_rate),]
missing_data_countries <-  df4$location
#missing_data_countries

```

```{r include=FALSE}

# Imputing life_expectancy missing values.

df4 <- df1[is.na(df1$life_expectancy),]
missing_data_countries <-  df4$location
#missing_data_countries

life_exp <- c(82.6, 80.6, 71.95)
position_in_vector <- which(df1$location %in% missing_data_countries)

df1$life_expectancy[position_in_vector] <-  life_exp

sum(is.na(df1$life_expectancy))
```


female and male smokers info not enough
handwashing facilities skipped because data is not enough.
hospital bed per thousand informatiion was found to be too old for example Chad which had data for 2005. Given that there are many years between the time the data was collected and this year the data was ignored.

Only three observations were found to have missing life expectancy values. Imputation was done using the world bank data on life expectancy [@LifeExpectancy]. The life expectancy of Guernsey, Jersey and Kosovo were found to be 82.6, 80.6, 71.95 years respectively.


No data was found on the internet woith regards to the rest of the variables on their missing data. There alternative methods had to be applied.
```{r echo=FALSE,  out.width = '100%', out.height='100%', fig.align="center" }
knitr::include_graphics("Capture.png")
```

With regards to population density, extreme poverty, gdp per capita, hospital beds per thousand, aged 65 and older and aged 70 and older, a better imputation method would be replacing missing values with column regional means given that countries in the same regions are more likely to have the same of these variables.


```{r include=FALSE}
# For the gdp per capita the missing values will be replaced by the mean for each region under the assumption that countries that are closer to each other usually have similar rates of poverty measures/gdp_per_capita, hospital beds per thousand, aged_65_older and aged_70_older


df6 <- df1 %>%
    group_by(continent) %>%
    mutate(
        population_density=ifelse(is.na(population_density),mean(population_density,na.rm=TRUE),population_density),
        extreme_poverty=ifelse(is.na(extreme_poverty),mean(extreme_poverty,na.rm=TRUE),extreme_poverty),
        gdp_per_capita=ifelse(is.na(gdp_per_capita),mean(gdp_per_capita,na.rm=TRUE),gdp_per_capita),
        hospital_beds_per_thousand=ifelse(is.na(hospital_beds_per_thousand),mean(hospital_beds_per_thousand,na.rm=TRUE),hospital_beds_per_thousand),
        aged_65_older=ifelse(is.na(aged_65_older),mean(aged_65_older,na.rm=TRUE),aged_65_older),
        aged_70_older=ifelse(is.na(aged_70_older),mean(aged_70_older,na.rm=TRUE),aged_70_older)
  
    )
```


Due to hand_washing_facilities having missing data greater than 70%, the variable was removed since it is most likely that the imputed values will be far from the actual values.

After imputation using the data scrapping and regional averages, the looks like below:
```{r}

# Remove handwashing_facilities since there is not enough 
# data for imputation

df7 <- select(df6,-handwashing_facilities)
aggr_plot <- VIM::aggr(df7, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df7), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"), plot = T)
plot(aggr_plot, cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

Now the highest percentage of missing values is found in male smokers and female smokers having 34% and 33 % missing values. For the these variables and other that have missing values different imputation algorithms will be explored and the one that gives the best results will be chosen.

```{r include=FALSE}

# Visualize the proportion of missing values in rows/columns

aggr_plot <- VIM::aggr(df7, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df7), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```
The results show that handwashing_facilites is missing 56.25% of the values followed by extreme_poverty missing 42.3% of the values. In total, 78.4% of the samples have missing values.











```{r include=FALSE}
df_correlation_plot <- df7 %>%
  keep(is.numeric)

correlation_matrix <- cor(df_correlation_plot)
corrplot(correlation_matrix, method="circle")
```

## Mice

The mice package allows multivariate imputation by chained equations. MICE assumes that the missing data are Missing at Random (MAR), which means that the propensity for a data point to be missing is not related to the missing data but is related to some of the observed data [@MissingData]. In this section, the MICE package will be used on the remaining variables with missing data that are cardiovasc_death_rate, diabetes prevalence, female and male smokers.

The number of multiple imputations was set to 5, method was set to classification and regression trees and the maximum number of iterations set to 500. 5 datasets were created and the goodness of fit is shown in the plots below. The graphs of imputed values(red) closely resembles that of the available values(blue). This shows that the imputation process was good.


```{r include=FALSE}


# Impute missing data using the mice package

#imputed_data <- mice(df7,m=5,maxit=50, method='cart', seed=500)
#summary(imputed_data)
#save(imputed_data, file = "replaced_missing_data.RData")
load("replaced_missing_data.RData")

sum(is.na(imputed_data)) # Check if imputations have been done
```

```{r}
# Check the quality of imputations

mice::densityplot(imputed_data)

```


Going forward the second dataset was randomly selected from the five datasets. This will not affect the outcome significantly given that the datasets have very similar properties. The continent, date and iso_code variables were removed from the dataset as they will not be useful in the final analysis. The dataset was named mice_data_for_clustering.

```{r include=FALSE}
# Select one of the imputed datasets

completedData <- mice::complete(imputed_data, 2)
#View(completedData)
#str(completedData)

# Remove isocode, continent and data ????
# Is it necessary continentt

df8 <- completedData[,c(-2,-3,-4)]
rownames(df8) <- df8$location

# Removing country names
mice_data_for_clustering <- df8[,-1]

```

## Hmisc
Hmisc is a multiple purpose package useful for data analysis, high â€“ level graphics, imputing missing values, advanced table making, model fitting & diagnostics (linear regression, logistic regression & cox regression) etc. Hmisc assumes linearity in the variables being predicted. The package was also used to impute values on the missing values.

mi (Multiple imputation with diagnostics) package provides several features for dealing with missing values. Like other packages, it also builds multiple imputation models to approximate missing values. And, uses predictive mean matching method like the mice package.

There is also the Amelia package and mi package that are used for imputation but these will not be considered for this analysis.







This will be support for dimension reduction.
```{r include=FALSE}

correlation_matrix <- cor(mice_data_for_clustering)
corrplot(correlation_matrix, method="circle")
```




The percentage of population aged_65_older looks to have regional patterns therefore it is justified to use regional medians/averages for imputation. Since aged_70_older can is closely related to aged_65_older it is decided to use the same method.



```{r include=FALSE}
# Impute missing data using the Hmisc package

df7.mis <- prodNA(df7, noNA = 0.1)

impute_arg <- aregImpute(~ cardiovasc_death_rate 
                         + diabetes_prevalence + female_smokers + male_smokers, data = df7.mis, n.impute = 5)

completeData2 <- impute.transcan(impute_arg, imputation=1, data=df7.mis, list.out=TRUE,pr=FALSE, check=FALSE) 

#head(completeData2)
#class(completeData2)

df_Hmisc = df7 # Make a copy of df7

df_Hmisc$cardiovasc_death_rate = completeData2$cardiovasc_death_rate
df_Hmisc$diabetes_prevalence = completeData2$diabetes_prevalence
df_Hmisc$female_smokers = completeData2$female_smokers
df_Hmisc$male_smokers = completeData2$male_smokers

df_Hmisc <- completedData[,c(-2,-3,-4)]
rownames(df_Hmisc) <- df_Hmisc$location

# Removing country names
df_Hmisc_for_clustering <- df_Hmisc[,-1]

#summary(df_Hmisc_for_clustering)
```

The imputations were made on the dataset and the resulting dataset was named df_Hmisc_for_clustering. The two datasets were used for clustering and the results were compared.


# Cluster Analysis

In cluster analysis, the number of clusters of interest is 6 given that we have six distinct regions in the data that are Africa, Asia, Europe, North America, Oceania and South America. In order to quantify the goodness of fit for each clustering method, the average silhouette method which determines the quality of the clustering. The optimum number of clusters in a dataset will have the maximum average silhouette width.

## Heirachical clustering

Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. In this excercise complete, single, average, median and centroid linkage were tested.


The results from heirachical clustering are shown below:




## K-means

## K-mediods

## Clara

## DBSCAN

# Dimension Reduction

## Results


```{r include=FALSE}
data.out <- mice_data_for_clustering
```


The randomizing, re-running, averaging and final run is a very good advice.




```{r }
# Hierarchical clustering
# Compute pairwise distance matrices
dist.out <- dist(data.out,
                 method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.out,
             method = "complete")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc_single <- hclust(dist.out,
             method = "single")
# Visualization of hclust
plot(hc_single, labels = F,-1)
plot(silhouette(cutree(hc_single,6),dist.out))



hc <- hclust(dist.out,
             method = "average")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "median")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))



hc <- hclust(dist.out,
             method = "centroid")
# Visualization of hclust
plot(hc, labels = F,-1)
plot(silhouette(cutree(hc,6),dist.out))





# # 
# rect.hclust(hc, k = 6, border = 2:3)
# 
# hcd <- as.dendrogram(hc)
# # Define nodePar
# nodePar <- list(lab.cex = 0.6,
#                 pch = c(20, 19),
#                 cex = 0.7,
#                 col = c("green","yellow"))
# plot(hcd,
#      xlab = "Height",
#      nodePar = nodePar,
#      main = "Cluster dendrogram",
#      edgePar = list(col = c("red","blue"), lwd = 2:1),
#      horiz = TRUE)
```


```{r}
hc_results <- read_excel("HeirachicalResults.xlsx")
knitr::kable(hc_results, digits = 5, caption = "Acronyms") %>%
  kable_styling(full_width = F, font_size = 7) %>%
  column_spec(1, border_left = T) %>%
  column_spec(2, border_right = T)

tt = tibble(Country = hc[["labels"]], Group = silhouette(cutree(hc_single,6),dist.out)[,1])

```

The results show that single linkage had the best results with a average silhouette width of 0.83. China 3, India 4, Indonesia 5, Pakistan 2, Brazil 2, US 6 are outliers in this data. The rest of the countries fall within the same group. Although the average silhouette width is high, the results do not demonstrate any regional patterns for the six continents.


```{r echo=FALSE}
 # clustering visualization
sub_grp <- cutree(hc, k = 6)
fviz_cluster(list(data = data.out,
                  cluster = sub_grp ))
```


```{r, echo=FALSE}
fviz_nbclust(data.out,
             FUN = hcut,
             method = "wss")
```

```{r include=FALSE}
c = fviz_nbclust(data.out,
                 FUN = hcut,
                 method = "silhouette")
c
```



```{r}

# Using k-means clustering
k.max <- 10

# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){

km.res <- kmeans(data.out, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(data.out))
sil[i] <- mean(ss[, 3])
}
# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

print(sil[6])
print(sil[2])
```
K-means clustering results show that the average silhouette width for six clusters is 0.677787 for 6 clusters. The diagram suggest that the best number of cluster in the data is 2 with an ASW of 0.9736113.


```{r }
# Using k-medoids clustering
pam.out <- pam(data.out, 6)
head(pam.out$cluster)
plot(pam.out)
```

```{r include=FALSE}
# Using Clara
clara.out <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clara.out), col = 2:3, main = "Silhouette plot")
```
The dataset is very small and this results in Clara performing worse than PAM.


```{r include=FALSE}
# Using DBSCAN
t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 47000, MinPts =3 )
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")
```




```{r include=FALSE}
data.out <- df_Hmisc_for_clustering 
```


The randomizing, re-running, averaging and final run is a very good advice.


```{r include=FALSE}

# Using k-means clustering
k.max <- 10

# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){

km.res <- kmeans(data.out, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(data.out))
sil[i] <- mean(ss[, 3])
}
# Plot the average silhouette width
plot(1:k.max,
     sil, type = "b",
     pch = 19,
     frame = FALSE,
     xlab = "Number of clusters k")

abline(v = which.max(sil), lty = 2)

print(sil[6])

```


```{r include=FALSE}
# Hierarchical clustering
# Compute pairwise distance matrices
dist.out <- dist(data.out,
                 method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.out,
             method = "complete")
# Visualization of hclust
plot(hc, labels = F,-1)
# 
rect.hclust(hc, k = 6, border = 2:3)

hcd <- as.dendrogram(hc)
# Define nodePar
nodePar <- list(lab.cex = 0.6,
                pch = c(20, 19),
                cex = 0.7,
                col = c("green","yellow"))
plot(hcd,
     xlab = "Height",
     nodePar = nodePar,
     main = "Cluster dendrogram",
     edgePar = list(col = c("red","blue"), lwd = 2:1),
     horiz = TRUE)
```



```{r include=FALSE}
 # clustering visualization
sub_grp <- cutree(hc, k = 6)
fviz_cluster(list(data = data.out,
                  cluster = sub_grp ))
```


```{r pressure, echo=FALSE}
fviz_nbclust(data.out,
             FUN = hcut,
             method = "wss")
```

```{r include=FALSE}
c = fviz_nbclust(data.out,
                 FUN = hcut,
                 method = "silhouette")
c
```


```{r include=FALSE}
# Using k-medoids clustering
pam.out <- pam(data.out, 6)
head(pam.out$cluster)
plot(pam.out)
```

```{r include=FALSE}
# Using Clara
clara.out <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clara.out), col = 2:3, main = "Silhouette plot")
```
The dataset is very small and this results in Clara performing worse than PAM.


```{r include=FALSE}
# Using DBSCAN
t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 47000, MinPts =3 )
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")
```



```{r include=FALSE}
# Attempt clustering after dimension reduction
# Using PCA

pca.out <- princomp(data.out, cor=T,scores=T)
pca.out
pca.out$loadings

Variance<-(pca.out$sdev)^2
max_Var<-round(max(Variance),1)
Components<-c(1:25)
Components<-as.integer(Components)
plot(Components,Variance,main="Scree Plot",xlab="Number of Components",ylab="Variance",type="o",col="blue",ylim=c(0,max_Var),axes=FALSE)
axis(1,at=1:25)
axis(2,at=0:10)

df10 =pca.out$scores
df11 =  df10[,c(1:5)]



# Average silhouette method for k-means clustering
k.max <- 10
data.out <- df11
# data.out <- df_Hmisc_for_clustering[-22,-23]
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 10
for(i in 2:k.max){

km.res <- kmeans(data.out, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(data.out))
sil[i] <- mean(ss[, 3])
}
# Plot the average silhouette width
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

print(sil[6])
print(sil[2])

# Partition around medoids
pam.out <- pam(data.out, 4)
head(pam.out$cluster)
plot(pam.out)

pam.out$silinfo$avg.width

k.max = 20
for (k in 2:k.max) {
  pam.out <- pam(data.out, k)
  sil[k] = pam.out$silinfo$avg.width
}

plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

print(sil[6])
print(sil[2])

# Using Clara
clarax <- clara(data.out, 6, samples=10)
# Silhouette plot
plot(silhouette(clarax), col = 2:3, main = "Silhouette plot")

clarax$silinfo$avg.width

for (k in 2:k.max) {
  pam.out <- pam(data.out, k)
  sil[k] = pam.out$silinfo$avg.width
}

plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)

t = scale(data.out)
db <- fpc::dbscan(data.out, eps = 2.7, MinPts = 1)
db[["cluster"]]
sil.dbscan <- silhouette(db$cluster, dist(t))
summary(sil.dbscan)
plot(sil.dbscan, col = 2:3, main = "Silhouette plot")

```


This code was optimised for reading


